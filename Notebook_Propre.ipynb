{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02ef76b5",
      "metadata": {},
      "source": [
        "# Modèle avancé - Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33640123",
      "metadata": {},
      "source": [
        "Nous avons vu qu'une limite du modèle XGBoost est le fait qu'il ne prenne pas en compte la **dimension séquentielle** des actions au cours du temps. \n",
        "\n",
        "Pour capturer les relations d'une séquence, différents modèles d'apprentissage profond ont vu le jour. Jusque la fin des années 2010, les méthodes de traitement de données séquentielles se basaient principalement sur de la récurrence (**réseaux de neurones récurrents**) ou de la convolution (**réseaux de neurones convolutifs**). Elles étaient ainsi couplées à un mécanisme d'attention, qui permet de déterminer l'importance de chaque élément de la séquence relativement à ses autres éléments.\n",
        "\n",
        "En 2017, le papier de recherche \"Attention Is All You Need\" (A. Vaswani, N. Shazeer, N. Parmar) introduit l'architecture **Transformer**, et révolutionne le traitement de données séquentielles. L'idée est de se baser exclusivement sur le mécanisme d'attention, ce qui permet une meilleure parallélisation des calculs, et un temps d'entraînement plus court.\n",
        "\n",
        "Dans ce Notebook, nous proposons une implémentation d'une architecture Transformer pour résoudre la prédiction des utilisateurs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0410bbc",
      "metadata": {},
      "source": [
        "## Sommaire\n",
        "\n",
        "- [Lecture des données](#lecture-des-données)\n",
        "- [Extraction des caractéristiques temporelles](#extraction-de-caractéristiques-temporelles)\n",
        "- [Tokenisation](#tokenisation)\n",
        "- [Sélection du modèle](#sélection-du-modèle)\n",
        "  - [Premier modèle : Transformer à un seul contexte](#premier-modèle--transformer-à-un-seul-contexte)\n",
        "  - [Deuxième stratégie : Transformer multi-contexte](#deuxième-stratégie--transformer-multi-contexte) \n",
        "- [Evaluation](#prédiction)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98235d3",
      "metadata": {},
      "source": [
        "## Lecture des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4bc3658a",
      "metadata": {
        "id": "4bc3658a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from model.reader import reader\n",
        "\n",
        "df = reader('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "579dba06",
      "metadata": {
        "id": "579dba06"
      },
      "outputs": [],
      "source": [
        "TARGET = df.iloc[:, 0]\n",
        "browsers = df.iloc[:, 1]\n",
        "sequence_lengths = df.iloc[:, 2]\n",
        "actions = df.iloc[:, 3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb11365",
      "metadata": {},
      "source": [
        "## Extraction de caractéristiques temporelles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101839bc",
      "metadata": {},
      "source": [
        "A l'aide des séquences d'actions et de la durée totale de la séquence, on extrait deux caractéristiques temporelles :\n",
        "- la durée de la session (en secondes) T\n",
        "- la vitesse d'action (en nombre d'actions par seconde), calculé selon n_actions / T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad39e5d7",
      "metadata": {
        "id": "ad39e5d7"
      },
      "outputs": [],
      "source": [
        "from model.time_features import bucketize_time_features, compute_time_features\n",
        "\n",
        "time_features = compute_time_features(actions, sequence_lengths)\n",
        "time_features = bucketize_time_features(time_features)\n",
        "\n",
        "duration_tokens = list(time_features['duration_bucket'])\n",
        "speed_tokens = list(time_features['speed_bucket'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0e612e",
      "metadata": {},
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1db3ac0d",
      "metadata": {
        "id": "1db3ac0d"
      },
      "outputs": [],
      "source": [
        "from model.tokenizer import tokenize_action_sequence, tokenize_browser_data, tokenize_username_data\n",
        "\n",
        "username_tokens, username_to_idx = tokenize_username_data(TARGET)\n",
        "action_tokens, action_to_idx = tokenize_action_sequence(actions)\n",
        "browser_tokens, browser_to_idx = tokenize_browser_data(browsers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab9d915",
      "metadata": {},
      "source": [
        "## Sélection du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7d488e5",
      "metadata": {},
      "source": [
        "Tout d'abord, prenons en charge CUDA pour entraîner le modèle sur un GPU NVIDIA T4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "498d6006",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "498d6006",
        "outputId": "3d424028-48ba-4e45-d0a0-2e0e38efc838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA L4\n",
            "GPU Memory: 23.8 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# GPU setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "    # Enable memory optimization\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "648973aa",
      "metadata": {},
      "source": [
        "L'entraînement de notre Transformer peut être résumé en quatre étapes :\n",
        "- On tokenize le texte, on ajoute des encodages de position et on préfixe par un ou plusieurs tokens de contexte pour conditionner la suite.\n",
        "- Les couches d’auto-attention + MLP calculent des représentations contextuelles, avec masquage pour ne pas “voir” le futur en auto-régressif.\n",
        "- La tête de sortie prédit la distribution du prochain token, puis on compare au vrai token via une perte d’entropie croisée.\n",
        "- On rétro-propage l’erreur et on met à jour les poids sur de grands époques jusqu’à convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02001a0",
      "metadata": {},
      "source": [
        "### Premier modèle : Transformer à un seul contexte"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c29ef0",
      "metadata": {},
      "source": [
        "La première architecture de Transformer que nous décidons d'entraînµer est un modèle à tête qui prend uniquement le token de contexte \"Browser\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6874946",
      "metadata": {},
      "source": [
        "Nous commençons par créer le modèle :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3da5012",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3da5012",
        "outputId": "56bfb157-4bd0-456a-fe0d-512415ef27bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 5,170,935\n"
          ]
        }
      ],
      "source": [
        "from model.transformer import create_model, train_model\n",
        "\n",
        "model = create_model(\n",
        "    vocab_size=len(action_to_idx),\n",
        "    n_usernames=len(username_to_idx),\n",
        "    n_browsers=len(browser_to_idx),\n",
        "    d_model=256,        \n",
        "    n_heads=8,          \n",
        "    n_layers=6,         \n",
        "    d_ff=512,\n",
        "    max_seq_len=500,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3608bc0",
      "metadata": {},
      "source": [
        "On réalise l'entraînement sur un GPU T4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33cadb36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33cadb36",
        "outputId": "a408b414-7b85-43b8-8daa-bb01bc700503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 2951 samples\n",
            "Validation on 328 samples\n",
            "Using max sequence length: 500\n",
            "Using batch size: 128\n",
            "Epoch   0: Train Loss: 5.7070, Val Loss: 5.6498, Val Acc: 0.0078, Val Macro-F1: 0.0001\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 105: 95.43% (313/328)\n",
            "    2. Username 143: 3.66% (12/328)\n",
            "    3. Username 17: 0.30% (1/328)\n",
            "    4. Username 163: 0.30% (1/328)\n",
            "    5. Username 62: 0.30% (1/328)\n",
            "Epoch  10: Train Loss: 5.3748, Val Loss: 5.4433, Val Acc: 0.0229, Val Macro-F1: 0.0086\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 143: 19.21% (63/328)\n",
            "    2. Username 159: 15.24% (50/328)\n",
            "    3. Username 165: 9.76% (32/328)\n",
            "    4. Username 237: 9.15% (30/328)\n",
            "    5. Username 118: 6.40% (21/328)\n",
            "Epoch  20: Train Loss: 4.9294, Val Loss: 5.0713, Val Acc: 0.0622, Val Macro-F1: 0.0370\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 57: 7.32% (24/328)\n",
            "    2. Username 62: 6.40% (21/328)\n",
            "    3. Username 15: 4.88% (16/328)\n",
            "    4. Username 159: 3.05% (10/328)\n",
            "    5. Username 56: 2.74% (9/328)\n",
            "Epoch  30: Train Loss: 4.3687, Val Loss: 4.5381, Val Acc: 0.1317, Val Macro-F1: 0.0872\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 15: 6.10% (20/328)\n",
            "    2. Username 62: 4.57% (15/328)\n",
            "    3. Username 213: 4.27% (14/328)\n",
            "    4. Username 199: 4.27% (14/328)\n",
            "    5. Username 237: 3.96% (13/328)\n",
            "Epoch  40: Train Loss: 3.8454, Val Loss: 4.0601, Val Acc: 0.1939, Val Macro-F1: 0.1377\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 25: 5.79% (19/328)\n",
            "    2. Username 15: 4.88% (16/328)\n",
            "    3. Username 213: 4.57% (15/328)\n",
            "    4. Username 237: 3.96% (13/328)\n",
            "    5. Username 27: 3.35% (11/328)\n",
            "Epoch  50: Train Loss: 3.3977, Val Loss: 3.6157, Val Acc: 0.2677, Val Macro-F1: 0.2060\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 25: 5.18% (17/328)\n",
            "    2. Username 96: 3.35% (11/328)\n",
            "    3. Username 213: 3.05% (10/328)\n",
            "    4. Username 232: 3.05% (10/328)\n",
            "    5. Username 15: 2.74% (9/328)\n",
            "Epoch  60: Train Loss: 2.9338, Val Loss: 3.2048, Val Acc: 0.3678, Val Macro-F1: 0.2816\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 96: 4.88% (16/328)\n",
            "    2. Username 25: 3.35% (11/328)\n",
            "    3. Username 39: 2.74% (9/328)\n",
            "    4. Username 24: 2.44% (8/328)\n",
            "    5. Username 155: 2.44% (8/328)\n",
            "Epoch  70: Train Loss: 2.5603, Val Loss: 2.8325, Val Acc: 0.4659, Val Macro-F1: 0.3726\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 96: 3.66% (12/328)\n",
            "    2. Username 25: 2.44% (8/328)\n",
            "    3. Username 9: 2.13% (7/328)\n",
            "    4. Username 199: 2.13% (7/328)\n",
            "    5. Username 24: 2.13% (7/328)\n",
            "Epoch  80: Train Loss: 2.1938, Val Loss: 2.4858, Val Acc: 0.5385, Val Macro-F1: 0.4351\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 96: 3.35% (11/328)\n",
            "    2. Username 9: 2.13% (7/328)\n",
            "    3. Username 21: 1.83% (6/328)\n",
            "    4. Username 219: 1.83% (6/328)\n",
            "    5. Username 24: 1.83% (6/328)\n",
            "Epoch  90: Train Loss: 1.8502, Val Loss: 2.1822, Val Acc: 0.6215, Val Macro-F1: 0.5480\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 96: 2.74% (9/328)\n",
            "    2. Username 9: 2.44% (8/328)\n",
            "    3. Username 219: 1.83% (6/328)\n",
            "    4. Username 17: 1.52% (5/328)\n",
            "    5. Username 28: 1.52% (5/328)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UsernameTransformer(\n",
              "  (token_embedding): Embedding(7087, 256, padding_idx=0)\n",
              "  (browser_embedding): Embedding(4, 256)\n",
              "  (username_embedding): Embedding(247, 256)\n",
              "  (pos_encoding): PositionalEncoding()\n",
              "  (transformer_blocks): ModuleList(\n",
              "    (0-5): 6 x TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (username_classifier): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=247, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split data (simple 80/20 split)\n",
        "split_idx = int(0.9 * len(df))\n",
        "train_data = (\n",
        "    action_tokens[:split_idx],\n",
        "    username_tokens[:split_idx],\n",
        "    browser_tokens[:split_idx]\n",
        ")\n",
        "val_data = (\n",
        "    action_tokens[split_idx:],\n",
        "    username_tokens[split_idx:],\n",
        "    browser_tokens[split_idx:]\n",
        ")\n",
        "\n",
        "train_model(\n",
        "    model,\n",
        "    train_data=train_data,\n",
        "    val_data=val_data,\n",
        "    epochs=100,\n",
        "    batch_size=128,      \n",
        "    max_seq_len=500,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c4f9f6",
      "metadata": {},
      "source": [
        "### Deuxième stratégie : Transformer multi-contexte"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d681628",
      "metadata": {},
      "source": [
        "Pour prendre en compte la dimension temporelle de nos données, nous décidons d'augmenter le contexte de deux tokens supplémentaires, extraits des jalons d\n",
        "temporels : la vitesse d'action de l'utilisateur et la durée de la session.\n",
        "\n",
        "Les têtes de lecture du modèle Transformer vont donc parcourir les séquences avec 3 tokens persistants en début de fenêtre : \"browser\", \"duration_bucket\", \"speed_bucket\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1204d426",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1204d426",
        "outputId": "59b749a3-0a9f-4096-8917-a28ad4eb1d60"
      },
      "outputs": [],
      "source": [
        "from model.transformer_extended_context import create_model, train_model\n",
        "\n",
        "discrete_contexts = {\n",
        "    'browser': 4,\n",
        "    'duration_bucket': 8,\n",
        "    'speed_bucket': 8\n",
        "}\n",
        "\n",
        "# Create optimized model for T4 GPU\n",
        "model = create_model(\n",
        "    vocab_size=len(action_to_idx),\n",
        "    n_usernames=len(username_to_idx),\n",
        "    discrete_contexts=discrete_contexts,\n",
        "    d_model=256,\n",
        "    n_heads=8,\n",
        "    n_layers=6,\n",
        "    d_ff=512,\n",
        "    max_seq_len=700,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf4e135",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bf4e135",
        "outputId": "9739a66c-9fd2-41a7-f6cc-a3bb4a70d669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 2623 samples\n",
            "Validation on 656 samples\n",
            "Using max sequence length: 700\n",
            "Using batch size: 64\n",
            "Epoch   0: Train Loss: 5.6829, Val Loss: 5.6145, Val Acc: 0.0014, Val Macro-F1: 0.0001\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 122: 27.90% (183/656)\n",
            "    2. Username 208: 24.54% (161/656)\n",
            "    3. Username 28: 19.82% (130/656)\n",
            "    4. Username 70: 13.57% (89/656)\n",
            "    5. Username 180: 4.42% (29/656)\n",
            "Epoch  10: Train Loss: 5.2546, Val Loss: 5.3007, Val Acc: 0.0341, Val Macro-F1: 0.0161\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 183: 13.87% (91/656)\n",
            "    2. Username 102: 8.23% (54/656)\n",
            "    3. Username 123: 7.47% (49/656)\n",
            "    4. Username 197: 7.32% (48/656)\n",
            "    5. Username 214: 6.40% (42/656)\n",
            "Epoch  20: Train Loss: 4.5544, Val Loss: 4.6627, Val Acc: 0.1051, Val Macro-F1: 0.0508\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 123: 6.86% (45/656)\n",
            "    2. Username 183: 5.18% (34/656)\n",
            "    3. Username 22: 4.57% (30/656)\n",
            "    4. Username 102: 4.57% (30/656)\n",
            "    5. Username 33: 4.12% (27/656)\n",
            "Epoch  30: Train Loss: 3.8541, Val Loss: 4.0083, Val Acc: 0.2301, Val Macro-F1: 0.1696\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 33: 4.57% (30/656)\n",
            "    2. Username 215: 3.66% (24/656)\n",
            "    3. Username 123: 3.66% (24/656)\n",
            "    4. Username 96: 3.51% (23/656)\n",
            "    5. Username 206: 3.35% (22/656)\n",
            "Epoch  40: Train Loss: 3.2209, Val Loss: 3.4326, Val Acc: 0.3523, Val Macro-F1: 0.2730\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 33: 3.51% (23/656)\n",
            "    2. Username 215: 2.44% (16/656)\n",
            "    3. Username 193: 2.44% (16/656)\n",
            "    4. Username 144: 2.29% (15/656)\n",
            "    5. Username 123: 2.13% (14/656)\n",
            "Epoch  50: Train Loss: 2.6383, Val Loss: 2.9023, Val Acc: 0.4616, Val Macro-F1: 0.3749\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 33: 2.44% (16/656)\n",
            "    2. Username 11: 2.44% (16/656)\n",
            "    3. Username 193: 2.29% (15/656)\n",
            "    4. Username 196: 1.98% (13/656)\n",
            "    5. Username 96: 1.83% (12/656)\n",
            "Epoch  60: Train Loss: 2.1388, Val Loss: 2.4389, Val Acc: 0.5795, Val Macro-F1: 0.4955\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 11: 2.90% (19/656)\n",
            "    2. Username 193: 1.83% (12/656)\n",
            "    3. Username 7: 1.83% (12/656)\n",
            "    4. Username 33: 1.83% (12/656)\n",
            "    5. Username 5: 1.68% (11/656)\n",
            "Epoch  70: Train Loss: 1.7259, Val Loss: 2.0734, Val Acc: 0.6463, Val Macro-F1: 0.5698\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 11: 3.35% (22/656)\n",
            "    2. Username 33: 1.68% (11/656)\n",
            "    3. Username 5: 1.68% (11/656)\n",
            "    4. Username 7: 1.52% (10/656)\n",
            "    5. Username 212: 1.37% (9/656)\n",
            "Epoch  80: Train Loss: 1.3761, Val Loss: 1.7830, Val Acc: 0.7216, Val Macro-F1: 0.6499\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 11: 3.51% (23/656)\n",
            "    2. Username 5: 1.83% (12/656)\n",
            "    3. Username 7: 1.52% (10/656)\n",
            "    4. Username 96: 1.37% (9/656)\n",
            "    5. Username 33: 1.37% (9/656)\n",
            "Epoch  90: Train Loss: 1.0757, Val Loss: 1.5484, Val Acc: 0.7543, Val Macro-F1: 0.6793\n",
            "  Top predicted usernames (by frequency):\n",
            "    1. Username 11: 3.51% (23/656)\n",
            "    2. Username 7: 1.98% (13/656)\n",
            "    3. Username 5: 1.52% (10/656)\n",
            "    4. Username 72: 1.37% (9/656)\n",
            "    5. Username 96: 1.22% (8/656)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UsernameTransformer(\n",
              "  (token_embedding): Embedding(7087, 256, padding_idx=0)\n",
              "  (context_embeddings): ModuleDict(\n",
              "    (browser): Embedding(4, 256)\n",
              "    (duration_bucket): Embedding(8, 256)\n",
              "    (speed_bucket): Embedding(8, 256)\n",
              "  )\n",
              "  (username_embedding): Embedding(247, 256)\n",
              "  (pos_encoding): PositionalEncoding()\n",
              "  (transformer_blocks): ModuleList(\n",
              "    (0-5): 6 x TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (w_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  (username_classifier): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.1, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=247, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "split_idx = int(0.8 * len(df))\n",
        "train_data = (\n",
        "    action_tokens[:split_idx],\n",
        "    username_tokens[:split_idx],\n",
        "    browser_tokens[:split_idx],\n",
        "    duration_tokens[:split_idx],\n",
        "    speed_tokens[:split_idx]\n",
        ")\n",
        "val_data = (\n",
        "    action_tokens[split_idx:],\n",
        "    username_tokens[split_idx:],\n",
        "    browser_tokens[split_idx:],\n",
        "    duration_tokens[split_idx:],\n",
        "    speed_tokens[split_idx:]\n",
        ")\n",
        "\n",
        "# Train with memory-efficient parameters\n",
        "train_model(\n",
        "    model,\n",
        "    train_data=train_data,\n",
        "    val_data=val_data,\n",
        "    epochs=100,\n",
        "    batch_size=64,      # Larger batch size for GPU\n",
        "    max_seq_len=700,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f0256b",
      "metadata": {},
      "source": [
        "## Prédiction - Challenge Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7578896c",
      "metadata": {
        "id": "7578896c"
      },
      "source": [
        "Use test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3357296f",
      "metadata": {
        "id": "3357296f"
      },
      "outputs": [],
      "source": [
        "test_df = reader('test.csv', training=False)\n",
        "\n",
        "browsers = test_df.iloc[:, 0]\n",
        "sequence_lengths = test_df.iloc[:, 1]\n",
        "actions = test_df.iloc[:, 2:]\n",
        "\n",
        "action_tokens, _ = tokenize_action_sequence(actions=actions, existing_token_to_idx=action_to_idx, training=False)\n",
        "browser_tokens, _ = tokenize_browser_data(browsers=browsers, existing_browser_to_idx=browser_to_idx, training=False)\n",
        "idx_to_username = {idx: username for username, idx in username_to_idx.items()}\n",
        "\n",
        "submission = []\n",
        "\n",
        "for i in range(len(action_tokens)):\n",
        "    action_sequence = torch.tensor(action_tokens[i]).to(device)\n",
        "    browser = torch.tensor(browser_tokens[i]).to(device)\n",
        "    logits, probs = model.predict_username(action_sequence, browser)\n",
        "    predicted_username = torch.argmax(logits, dim=-1)\n",
        "    predicted_idx = predicted_username.item()\n",
        "    predicted_username_name = idx_to_username[predicted_idx]\n",
        "    submission.append(predicted_username_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e5a2e6",
      "metadata": {},
      "source": [
        "Sauvegarde de la prédiction en CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9edcdb6d",
      "metadata": {
        "id": "9edcdb6d"
      },
      "outputs": [],
      "source": [
        "df_subm = pd.DataFrame({\"prediction\": submission})\n",
        "df_subm = df_subm.rename_axis(\"RowId\")\n",
        "df_subm.index = df_subm.index + 1\n",
        "\n",
        "df_subm.to_csv(\"submission_5.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13329a6f",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78347b7d",
      "metadata": {},
      "source": [
        "L'entraînement d'un modèle Transformer présente des avantages (capture de l'ordre de la séquence), mais aussi des inconvénients.\n",
        "\n",
        "\n",
        "Notre jeu de données présente peu d'exemples par utilisateur, ce qui rend l'utilisation du deep learning propice à l'overfitting. En particulier, notre jeu de données présente un déséquilibre de classes, avec un nombre de session d'utilisateur entre 4 et 72. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
