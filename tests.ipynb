{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43132f07",
   "metadata": {},
   "source": [
    "# Pipeline Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d8739",
   "metadata": {},
   "source": [
    "## 1. Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc3658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model.reader import reader\n",
    "\n",
    "df = reader('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579dba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = df.iloc[:, 0]\n",
    "browsers = df.iloc[:, 1]\n",
    "sequence_lengths = df.iloc[:, 2]\n",
    "actions = df.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07944e89",
   "metadata": {},
   "source": [
    "## Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad39e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.time_features import bucketize_time_features, compute_time_features\n",
    "\n",
    "time_features = compute_time_features(actions, sequence_lengths)\n",
    "time_features = bucketize_time_features(time_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80064f",
   "metadata": {},
   "source": [
    "## 2. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db3ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tokenizer import tokenize_action_sequence, tokenize_browser_data, tokenize_username_data\n",
    "\n",
    "username_tokens, username_to_idx = tokenize_username_data(TARGET)\n",
    "action_tokens, action_to_idx = tokenize_action_sequence(actions)\n",
    "browser_tokens, browser_to_idx = tokenize_browser_data(browsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca69ff2",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c9ad9",
   "metadata": {},
   "source": [
    "Read device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498d6006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Enable memory optimization\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa0a2b",
   "metadata": {},
   "source": [
    "### Model: Transformer with context token \"Browser\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a98e83",
   "metadata": {},
   "source": [
    "Create new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.transformer import create_model, train_model\n",
    "\n",
    "# Create optimized model for T4 GPU\n",
    "model = create_model(\n",
    "    vocab_size=len(action_to_idx),\n",
    "    n_usernames=len(username_to_idx),\n",
    "    n_browsers=len(browser_to_idx),\n",
    "    d_model=256,        # Increased for better performance\n",
    "    n_heads=8,          # More attention heads\n",
    "    n_layers=6,         # Deeper network\n",
    "    d_ff=512,\n",
    "    max_seq_len=100,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01baa8b9",
   "metadata": {},
   "source": [
    "Train Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cadb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the raw session_tokens (list of lists) instead of converting to tensor\n",
    "# This allows the training function to handle padding properly\n",
    "\n",
    "# Split data (simple 80/20 split)\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_data = (\n",
    "    action_tokens[:split_idx],\n",
    "    username_tokens[:split_idx],\n",
    "    browser_tokens[:split_idx]\n",
    ")\n",
    "val_data = (\n",
    "    action_tokens[split_idx:],\n",
    "    username_tokens[split_idx:],\n",
    "    browser_tokens[split_idx:]\n",
    ")\n",
    " \n",
    "# Train with memory-efficient parameters\n",
    "train_model(\n",
    "    model,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    epochs=100,\n",
    "    batch_size=32,      # Larger batch size for GPU\n",
    "    max_seq_len=500,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcde419",
   "metadata": {},
   "source": [
    "Alternatively: import saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d50f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.save_load_transformer import load_model\n",
    "\n",
    "\n",
    "# model, metadata = load_model(filepath=\"saved_models/transformer1.pt\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7be97b",
   "metadata": {},
   "source": [
    "### Model: Transformer with context token \"Browser\", \"Duration\" and \"Speed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1204d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 5,175,031\n"
     ]
    }
   ],
   "source": [
    "from model.transformer_extended_context import create_model, train_model\n",
    "\n",
    "discrete_contexts = {\n",
    "    'browser': 4,\n",
    "    'duration_bucket': 8,\n",
    "    'speed_bucket': 8\n",
    "}\n",
    "\n",
    "# Create optimized model for T4 GPU\n",
    "model = create_model(\n",
    "    vocab_size=len(action_to_idx),\n",
    "    n_usernames=len(username_to_idx),\n",
    "    discrete_contexts=discrete_contexts,\n",
    "    d_model=256,       \n",
    "    n_heads=8,          \n",
    "    n_layers=6,        \n",
    "    d_ff=512,\n",
    "    max_seq_len=100,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf4e135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2623 samples\n",
      "Validation on 656 samples\n",
      "Using max sequence length: 500\n",
      "Using batch size: 32\n",
      "Epoch   0: Train Loss: 5.6413, Val Loss: 5.5648, Val Acc: 0.0000\n",
      "  Top predicted usernames (by frequency):\n",
      "    1. Username 128: 47.41% (311/656)\n",
      "    2. Username 183: 35.98% (236/656)\n",
      "    3. Username 143: 8.38% (55/656)\n",
      "    4. Username 126: 3.66% (24/656)\n",
      "    5. Username 34: 1.37% (9/656)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     12\u001b[39m val_data = (\n\u001b[32m     13\u001b[39m     action_tokens[split_idx:],\n\u001b[32m     14\u001b[39m     username_tokens[split_idx:],\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     speed_tokens[split_idx:]\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train with memory-efficient parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Larger batch size for GPU\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\model\\transformer_extended_context.py:529\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_data, val_data, learning_rate, epochs, batch_size, max_seq_len, device)\u001b[39m\n\u001b[32m    526\u001b[39m val_batch_duration_buckets = torch.tensor(val_batch_duration_buckets, dtype=torch.long) \u001b[38;5;28;01mif\u001b[39;00m val_batch_duration_buckets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    527\u001b[39m val_batch_speed_buckets = torch.tensor(val_batch_speed_buckets, dtype=torch.long) \u001b[38;5;28;01mif\u001b[39;00m val_batch_speed_buckets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m batch_loss, batch_accuracy, batch_predictions = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_batch_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_usernames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_browsers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_duration_buckets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_speed_buckets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m val_loss += batch_loss\n\u001b[32m    531\u001b[39m val_accuracy += batch_accuracy\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\model\\transformer_extended_context.py:373\u001b[39m, in \u001b[36mUsernameTransformerTrainer.evaluate\u001b[39m\u001b[34m(self, action_sequences, usernames, browsers, duration_buckets, speed_buckets)\u001b[39m\n\u001b[32m    370\u001b[39m speed_buckets = speed_buckets.to(\u001b[38;5;28mself\u001b[39m.device) \u001b[38;5;28;01mif\u001b[39;00m speed_buckets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     username_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrowsers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration_buckets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeed_buckets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.criterion(username_logits, usernames)\n\u001b[32m    376\u001b[39m     predictions = torch.argmax(username_logits, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\model\\transformer_extended_context.py:262\u001b[39m, in \u001b[36mUsernameTransformer.forward\u001b[39m\u001b[34m(self, action_sequence, browser, duration_bucket, speed_bucket, username, training)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Pass through transformer blocks\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m transformer_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     x = \u001b[43mtransformer_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# Apply layer norm\u001b[39;00m\n\u001b[32m    265\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_norm(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\model\\transformer_extended_context.py:116\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    115\u001b[39m     \u001b[38;5;66;03m# Self-attention with residual connection and layer norm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(x + \u001b[38;5;28mself\u001b[39m.dropout(attn_output))\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# Feed-forward with residual connection and layer norm\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aurel\\Projects\\centrale-4a\\introduction_data_science\\model\\transformer_extended_context.py:77\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask)\u001b[39m\n\u001b[32m     75\u001b[39m Q = \u001b[38;5;28mself\u001b[39m.w_q(query).view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.d_k).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m     76\u001b[39m K = \u001b[38;5;28mself\u001b[39m.w_k(key).view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_heads, \u001b[38;5;28mself\u001b[39m.d_k).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m V = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mw_v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43md_k\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Apply attention\u001b[39;00m\n\u001b[32m     80\u001b[39m attention_output, attention_weights = \u001b[38;5;28mself\u001b[39m.scaled_dot_product_attention(Q, K, V, mask)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "duration_tokens = list(time_features['duration_bucket'])\n",
    "speed_tokens = list(time_features['speed_bucket'])\n",
    "\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_data = (\n",
    "    action_tokens[:split_idx],\n",
    "    username_tokens[:split_idx],\n",
    "    browser_tokens[:split_idx],\n",
    "    duration_tokens[:split_idx],\n",
    "    speed_tokens[:split_idx]\n",
    ")\n",
    "val_data = (\n",
    "    action_tokens[split_idx:],\n",
    "    username_tokens[split_idx:],\n",
    "    browser_tokens[split_idx:],\n",
    "    duration_tokens[split_idx:],\n",
    "    speed_tokens[split_idx:]\n",
    ")\n",
    " \n",
    "# Train with memory-efficient parameters\n",
    "train_model(\n",
    "    model,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    epochs=100,\n",
    "    batch_size=32,      # Larger batch size for GPU\n",
    "    max_seq_len=500,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b5953",
   "metadata": {},
   "source": [
    "## Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578896c",
   "metadata": {},
   "source": [
    "Use test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reader('test.csv', training=False)\n",
    "\n",
    "browsers = test_df.iloc[:, 0]\n",
    "sequence_lengths = test_df.iloc[:, 1]\n",
    "actions = test_df.iloc[:, 2:]\n",
    "\n",
    "action_tokens, _ = tokenize_action_sequence(actions=actions, existing_token_to_idx=action_to_idx, training=False)\n",
    "browser_tokens, _ = tokenize_browser_data(browsers=browsers, existing_browser_to_idx=browser_to_idx, training=False)\n",
    "idx_to_username = {idx: username for username, idx in username_to_idx.items()}\n",
    "\n",
    "submission = []\n",
    "\n",
    "for i in range(len(action_tokens)):\n",
    "    action_sequence = torch.tensor(action_tokens[i]).to(device)\n",
    "    browser = torch.tensor(browser_tokens[i]).to(device)\n",
    "    logits, probs = model.predict_username(action_sequence, browser)\n",
    "    predicted_username = torch.argmax(logits, dim=-1)\n",
    "    predicted_idx = predicted_username.item()\n",
    "    predicted_username_name = idx_to_username[predicted_idx]\n",
    "    submission.append(predicted_username_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
