{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43132f07",
   "metadata": {},
   "source": [
    "# Modèle avancé - Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb15c50d",
   "metadata": {},
   "source": [
    "Nous avons vu qu'une limite du modèle XGBoost est le fait qu'il ne prenne pas en compte la **dimension séquentielle** des actions au cours du temps. \n",
    "\n",
    "Pour capturer les relations d'une séquence, différents modèles d'apprentissage profond ont vu le jour. Jusque la fin des années 2010, les méthodes de traitement de données séquentielles se basaient principalement sur de la récurrence (**réseaux de neurones récurrents**) ou de la convolution (**réseaux de neurones convolutifs**). Elles étaient ainsi couplées à un mécanisme d'attention, qui permet de déterminer l'importance de chaque élément de la séquence relativement à ses autres éléments.\n",
    "\n",
    "En 2017, le papier de recherche \"Attention Is All You Need\" (A. Vaswani, N. Shazeer, N. Parmar) introduit l'architecture **Transformer**, et révolutionne le traitement de données séquentielles. L'idée est de se baser exclusivement sur le mécanisme d'attention, ce qui permet une meilleure parallélisation des calculs, et un temps d'entraînement plus court.\n",
    "\n",
    "Dans ce Notebook, nous proposons une implémentation d'une architecture Transformer pour résoudre la prédiction des utilisateurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cb8a3",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "- [Lecture des données](#lecture-des-données)\n",
    "- [Extraction des caractéristiques temporelles](#extraction-de-caractéristiques-temporelles)\n",
    "- [Tokenisation](#tokenisation)\n",
    "- [Sélection du modèle](#sélection-du-modèle)\n",
    "  - [Premier modèle : Transformer à un seul contexte](#premier-modèle--transformer-à-un-seul-contexte)\n",
    "  - [Deuxième stratégie : Transformer multi-contexte](#deuxième-stratégie--transformer-multi-contexte) \n",
    "- [Evaluation](#prédiction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d8739",
   "metadata": {},
   "source": [
    "## Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model.reader import reader\n",
    "\n",
    "df = reader('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = df.iloc[:, 0]\n",
    "browsers = df.iloc[:, 1]\n",
    "sequence_lengths = df.iloc[:, 2]\n",
    "actions = df.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07944e89",
   "metadata": {},
   "source": [
    "## Extraction de caractéristiques temporelles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a7bc4",
   "metadata": {},
   "source": [
    "A l'aide des séquences d'actions et de la durée totale de la séquence, on extrait deux caractéristiques temporelles :\n",
    "- la durée de la session (en secondes) T\n",
    "- la vitesse d'action (en nombre d'actions par seconde), calculé selon n_actions / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.time_features import bucketize_time_features, compute_time_features\n",
    "\n",
    "time_features = compute_time_features(actions, sequence_lengths)\n",
    "time_features = bucketize_time_features(time_features)\n",
    "\n",
    "duration_tokens = list(time_features['duration_bucket'])\n",
    "speed_tokens = list(time_features['speed_bucket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80064f",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.tokenizer import tokenize_action_sequence, tokenize_browser_data, tokenize_username_data\n",
    "\n",
    "username_tokens, username_to_idx = tokenize_username_data(TARGET)\n",
    "action_tokens, action_to_idx = tokenize_action_sequence(actions)\n",
    "browser_tokens, browser_to_idx = tokenize_browser_data(browsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca69ff2",
   "metadata": {},
   "source": [
    "## Sélection du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c9ad9",
   "metadata": {},
   "source": [
    "Tout d'abord, prenons en charge CUDA pour entraîner le modèle sur un GPU NVIDIA T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498d6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# GPU setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    # Enable memory optimization\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a8a84",
   "metadata": {},
   "source": [
    "L'entraînement de notre Transformer peut être résumé en quatre étapes :\n",
    "- On tokenize le texte, on ajoute des encodages de position et on préfixe par un ou plusieurs tokens de contexte pour conditionner la suite.\n",
    "- Les couches d’auto-attention + MLP calculent des représentations contextuelles, avec masquage pour ne pas “voir” le futur en auto-régressif.\n",
    "- La tête de sortie prédit la distribution du prochain token, puis on compare au vrai token via une perte d’entropie croisée.\n",
    "- On rétro-propage l’erreur et on met à jour les poids sur de grands époques jusqu’à convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa0a2b",
   "metadata": {},
   "source": [
    "### Premier modèle : Transformer à un seul contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03dc8f",
   "metadata": {},
   "source": [
    "La première architecture de Transformer que nous décidons d'entraînµer est un modèle à tête qui prend uniquement le token de contexte \"Browser\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a98e83",
   "metadata": {},
   "source": [
    "Nous commençons par créer le modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.transformer import create_model, train_model\n",
    "\n",
    "model = create_model(\n",
    "    vocab_size=len(action_to_idx),\n",
    "    n_usernames=len(username_to_idx),\n",
    "    n_browsers=len(browser_to_idx),\n",
    "    d_model=256,        \n",
    "    n_heads=8,          \n",
    "    n_layers=6,         \n",
    "    d_ff=512,\n",
    "    max_seq_len=500,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01baa8b9",
   "metadata": {},
   "source": [
    "On réalise l'entraînement sur un GPU T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cadb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (simple 80/20 split)\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_data = (\n",
    "    action_tokens[:split_idx],\n",
    "    username_tokens[:split_idx],\n",
    "    browser_tokens[:split_idx]\n",
    ")\n",
    "val_data = (\n",
    "    action_tokens[split_idx:],\n",
    "    username_tokens[split_idx:],\n",
    "    browser_tokens[split_idx:]\n",
    ")\n",
    " \n",
    "train_model(\n",
    "    model,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    epochs=100,\n",
    "    batch_size=32,      \n",
    "    max_seq_len=500,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7be97b",
   "metadata": {},
   "source": [
    "### Deuxième stratégie : Transformer multi-contexte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12cb59",
   "metadata": {},
   "source": [
    "Pour prendre en compte la dimension temporelle de nos données, nous décidons d'augmenter le contexte de deux tokens supplémentaires, extraits des jalons d\n",
    "temporels : la vitesse d'action de l'utilisateur et la durée de la session.\n",
    "\n",
    "Les têtes de lecture du modèle Transformer vont donc parcourir les séquences avec 3 tokens persistants en début de fenêtre : \"browser\", \"duration_bucket\", \"speed_bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.transformer_extended_context import create_model, train_model\n",
    "\n",
    "discrete_contexts = {\n",
    "    'browser': 4,\n",
    "    'duration_bucket': 8,\n",
    "    'speed_bucket': 8\n",
    "}\n",
    "\n",
    "# Create optimized model for T4 GPU\n",
    "model = create_model(\n",
    "    vocab_size=len(action_to_idx),\n",
    "    n_usernames=len(username_to_idx),\n",
    "    discrete_contexts=discrete_contexts,\n",
    "    d_model=256,       \n",
    "    n_heads=8,          \n",
    "    n_layers=6,        \n",
    "    d_ff=512,\n",
    "    max_seq_len=100,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(0.8 * len(df))\n",
    "train_data = (\n",
    "    action_tokens[:split_idx],\n",
    "    username_tokens[:split_idx],\n",
    "    browser_tokens[:split_idx],\n",
    "    duration_tokens[:split_idx],\n",
    "    speed_tokens[:split_idx]\n",
    ")\n",
    "val_data = (\n",
    "    action_tokens[split_idx:],\n",
    "    username_tokens[split_idx:],\n",
    "    browser_tokens[split_idx:],\n",
    "    duration_tokens[split_idx:],\n",
    "    speed_tokens[split_idx:]\n",
    ")\n",
    " \n",
    "# Train with memory-efficient parameters\n",
    "train_model(\n",
    "    model,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    epochs=100,\n",
    "    batch_size=32,      # Larger batch size for GPU\n",
    "    max_seq_len=500,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414fe1c",
   "metadata": {},
   "source": [
    "### Troisième stratégie : Transformer avec Embeddings\n",
    "\n",
    "On crée des embeddings pour \"browser\", \"duration\", \"speed\", et aussi pour les statistiques de distribution des tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d156e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.transformer_3 import create_model, train_model\n",
    "\n",
    "discrete_contexts = {\n",
    "    'browser': 4,\n",
    "    'duration_bucket': 8,\n",
    "    'speed_bucket': 8\n",
    "}\n",
    "\n",
    "# Create optimized model for T4 GPU\n",
    "model = create_model(\n",
    "    vocab_size=len(action_to_idx),\n",
    "    n_usernames=len(username_to_idx),\n",
    "    discrete_contexts=discrete_contexts,\n",
    "    d_model=256,       \n",
    "    n_heads=8,          \n",
    "    n_layers=6,        \n",
    "    d_ff=512,\n",
    "    max_seq_len=2000,\n",
    "    dropout=0.1,\n",
    "    context_as_features=True,\n",
    "    use_token_statistics=True,\n",
    "    token_stats_top_k=10\n",
    ")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "split_idx = int(0.8 * len(df))\n",
    "train_data = (\n",
    "    action_tokens[:split_idx],\n",
    "    username_tokens[:split_idx],\n",
    "    browser_tokens[:split_idx],\n",
    "    duration_tokens[:split_idx],\n",
    "    speed_tokens[:split_idx]\n",
    ")\n",
    "val_data = (\n",
    "    action_tokens[split_idx:],\n",
    "    username_tokens[split_idx:],\n",
    "    browser_tokens[split_idx:],\n",
    "    duration_tokens[split_idx:],\n",
    "    speed_tokens[split_idx:]\n",
    ")\n",
    " \n",
    "# Train with memory-efficient parameters\n",
    "train_model(\n",
    "    model,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    epochs=100,\n",
    "    batch_size=32,     \n",
    "    max_seq_len=2000,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b5953",
   "metadata": {},
   "source": [
    "## Prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578896c",
   "metadata": {},
   "source": [
    "Use test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3357296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = reader('test.csv', training=False)\n",
    "\n",
    "browsers = test_df.iloc[:, 0]\n",
    "sequence_lengths = test_df.iloc[:, 1]\n",
    "actions = test_df.iloc[:, 2:]\n",
    "\n",
    "action_tokens, _ = tokenize_action_sequence(actions=actions, existing_token_to_idx=action_to_idx, training=False)\n",
    "browser_tokens, _ = tokenize_browser_data(browsers=browsers, existing_browser_to_idx=browser_to_idx, training=False)\n",
    "idx_to_username = {idx: username for username, idx in username_to_idx.items()}\n",
    "\n",
    "submission = []\n",
    "\n",
    "for i in range(len(action_tokens)):\n",
    "    action_sequence = torch.tensor(action_tokens[i]).to(device)\n",
    "    browser = torch.tensor(browser_tokens[i]).to(device)\n",
    "    logits, probs = model.predict_username(action_sequence, browser)\n",
    "    predicted_username = torch.argmax(logits, dim=-1)\n",
    "    predicted_idx = predicted_username.item()\n",
    "    predicted_username_name = idx_to_username[predicted_idx]\n",
    "    submission.append(predicted_username_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebee44",
   "metadata": {},
   "source": [
    "Sauvegarde de la prédiction en CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc880aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm = pd.DataFrame({\"prediction\": submission})\n",
    "df_subm = df_subm.rename_axis(\"RowId\")\n",
    "df_subm.index = df_subm.index + 1\n",
    "\n",
    "df_subm.to_csv(\"output/submission_5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805aabd",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3c523",
   "metadata": {},
   "source": [
    "L'entraînement d'un modèle Transformer présente des avantages (capture de l'ordre de la séquence), mais aussi des inconvénients.\n",
    "\n",
    "\n",
    "Notre jeu de données présente peu d'exemples par utilisateur, ce qui rend l'utilisation du deep learning propice à l'overfitting. En particulier, notre jeu de données présente un déséquilibre de classes, avec un nombre de session d'utilisateur entre 4 et 72. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
